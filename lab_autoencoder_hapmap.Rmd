---
title: "Autoencoder Lab in R"
subtitle: "visualizing HapMap phase 3 populations"
author: "`r paste0('Marcin Kierczak | <b>NBIS</b> | ',format(Sys.time(), '%d-%b-%Y'))`"
output:
  bookdown::html_document2:
          toc: true
          toc_float: true
          toc_depth: 4
          number_sections: true
          theme: flatly
          highlight: tango
          df_print: paged
          code_folding: "none"
          self_contained: false
          keep_md: false
          encoding: 'UTF-8'
          css: "assets/lab.css"
---

```{r,child="assets/header-lab.Rmd"}
```
<!-- ------------ Only edit title, subtitle & author above this ------------ -->

```{r,include=FALSE}
#load the packages you need

#library(dplyr)
#library(tidyr)
#library(stringr)
#library(ggplot2)
```

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Synopsis
In this lab, we will use chip genotyping data from Hap Map phase 3 project. These data come from a number of humans belonging to different ethnic groups/populations. The groups are genetically distinct but, in some cases, closely related and thus somewhat difficult to distinguish. We will first try to visualize population structure using classical dimensionality reduction techniques like PCA and MDS and next, we will build autoencoder and see if it does any better in separating different populations. We will work in R and use `keras` interface to `TensorFlow`.

# Working environment

Before we begin, we have to set up a proper working `r emo::ji('factory')` environment. Follow the points below on your way to success. `r emo::ji('smile')`

* Install R package `renv` that will manage your working environment in a way similar to `Conda` `r emo::ji('snake')`. It is a bit smarter than `Conda`, since libraries can be shared between projects, so using it across your projects will (hopefully) not result in disk space shortage `r emo::ji('confused')`.
```{r install_renv, eval=F} 
install.packages('renv')
```

* [Download my `renv.lock`](assets/renv.lock) file. It is a text file that tells the `renv` package how to re-create the environment. Place the file in your working `r emo::ji('file_folder')`.

* Re-create the environment from the `renv.lock` snapshot. 
```{r renv_restore, eval=FALSE}
renv::restore(lockfile = 'renv.lock')
```

* Install `keras`. Keras will be installed by the R `keras` package that will, in turn, use either `conda` or `virtualenv`. It will also install TensorFlow for you.
```{r install_keras, eval=FALSE}
keras::install_keras()
```

* Congratulate yourself! `r emo::ji("biceps")`

# Background

The purpose of this lab is to evaluate the possibility of using autoencoder as a replacement/complement to more "classical" linear dimensionality reduction methods such as PCA or MDS. These are commonly used for, e.g. visualizing population structure in genetics. One of the main motivations is that when inferring genomic kinship from a large number of markers `M` (large enough to capture population structure at fine level), one necessarily introduces correlations between variables, here, genetic markers. This is predominantly due to the linkage disequilibrium, but also due to the large `M` that, even by pure chance, introduces correlated variables to the data. This correlation structure introduces non-linearity that, in turn, makes the data not very well suitable for PCA/MDS since both approaches rely on computing kinship matrix determinants that, for a lot of highly correlated variables, become 0 and prevent us from computing exact solutions (division by zero is undefined). 

Here, the working hypotheses is that by choosing non-linear activation functions, e.g. ReLU, one can circumvent this problem and use autoencoder approach to reduce the dimensionality by embedding kinship data in a low dimensional latent representation space that, in turn, can easily be visualized. The idea emerged during the EMBL conference *Reconstructing the Human Past*, Heidelberg `r emo::ji('beer')`, April 2019, in a number of discussions with Nikolay Oskolkov `r emo::ji('man_scientist')` and other conference participants: `r emo::ji('squirrel')`, `r emo::ji('zebra')` and `r emo::ji('dragon')`.

## Data
Data comes from the HapMap phase 3 project. Here, for computational feasibility, we will be using smaller dataset. I have pre-selected 5,000 autosomal markers with call rate of 100%. We will not be dealing with missing data here although autoencoders, in contrast to PCA and MDS, can. 

HapMap 3 populations:

* ASW --	African `r emo::ji('Africa')` ancestry in Southwest USA `r emo::flag('United States')`
* CEU --	Utah residents with Northern and Western European `r emo::ji('Europe')` ancestry from the CEPH collection
* CHB --	Han Chinese in Beijing, China `r emo::flag('^China$')`
* CHD --	Chinese `r emo::flag('^China$')` in Metropolitan Denver, Colorado `r emo::ji('mountain')`
* GIH --	Gujarati Indians `r emo::flag('^India$')` in Houston, Texas
* JPT --	Japanese in Tokyo, Japan `r emo::flag('Japan')`
* LWK --	Luhya in Webuye, Kenya `r emo::flag('Kenya')`
* MEX --	Mexican `r emo::flag('Mexico')` ancestry in Los Angeles, California `r emo::ji('bear')`
* MKK --	Maasai in Kinyawa, Kenya `r emo::flag('Kenya')`
* TSI --	Toscans `r emo::ji('motor_scooter')` in Italy `r emo::flag('Italy')`
* YRI --	Yoruba in Ibadan, Nigeria `r emo::flag('Nigeria')`

# Preparations
First, we need to [download the `autosomal_5k.rdat` dataset](assets/autosomal_5k.rdat). Data are stored as an R data object, more specifically, a `GenABEL::gwaa-data` class object consisting of 1184 individuals, each genotyped at 5000 loci. The loci are randomly spread across autosomes. If anyone is curious, the code below was used to generate this subset of the original dataset. 
```{r data_preparation, eval=FALSE}
data <- load.gwaa.data("hapmap3_r2_b36_fwd.consensus.qc.poly.csv", "hapmap3_r2_b36_fwd.consensus.qc.poly.out")
data_auto <- data[, autosomal(data)]
rm(data)
snp_subset <- sample(1:nsnps(data_auto), size = 50000, replace = F)
data_auto <- data_auto[,snp_subset]
qc1 <- check.marker(data_auto, callrate = 1.0)
data_autosomal <- data_auto[qc1$idok, qc1$snpok]
snp_subset <- sample(1:nsnps(data_autosomal), size = 5000, replace = F)
data_autosomal <- data_autosomal[,snp_subset]
save(data_autosomal, file = "./autosomal_5k.rdat")
```

We will begin by setting your working directory and loading necessary packages (they should be automatically installed when you restored my environment using `renv::restore()`). 

```{r echo=T, include=T, cache=FALSE, message=F}
library(renv)
library(here)
library(GenABEL)
library(keras)
library(kerasR)
library(ggplot2)
library(ggbiplot)

here::here() # check what is our current working directory
base::load(here::here('assets/autosomal_5k.rdat'))
```

# Benchmark

First, to have some sort of a benchmark `r emo::ji('straight_ruler')`, we will do PCA and MDS (which should be more or less equivalent) on the genomic kinship matrix to visualize patterns present in the data.

```{r cache = T}
# Compute genomic kinship-based distance
gkin <- ibs(data_autosomal, weight = 'freq')
dm <- as.dist(.5 - gkin) # Normalize it
```

## PCA
```{r perform_pca, cache=T}
pca <- stats::prcomp(dm)
g <- ggbiplot(pca, obs.scale = 1, var.scale = 1, 
              groups = data_autosomal@phdata$population, ellipse = F, 
              circle = TRUE, var.axes = F) + 
  scale_color_discrete(name = '') + 
  theme(legend.direction = 'horizontal', 
               legend.position = 'top') + 
  theme_bw()
print(g)
```

## MDS
```{r perform_mds, cache = T}
ibs <- as.data.frame(cmdscale(dm))
ibs <- cbind(ibs, pop = data_autosomal@phdata$population)
ggplot(ibs, mapping = aes(x=V1, y=V2, col=pop)) + 
  geom_point() + 
  theme_bw()
```

# Autoencoder

## Model parameters
Below, we define model parameters: loss function `r emo::ji('chart_with_upwards_trend')` set to the mean squared error and activation layer set to ReLU. One can refer to [Keras docs](https://keras.io/api/metrics/) for more insights.

```{r}
loss_fn <- 'mean_squared_error'
act <- 'relu'
```

## Prepare input
Input data is first normalized so that:

* homozygotes `AA` are set to 1
* heterozygotes `aA` and `Aa` to 0.5 and 
* homozygotes `aa` to 0.

Next, the data are randomly `r emo::ji('dice')` split into the validation (20%) and the training (80%) set.

```{r cache=T}
# Encode genotypes 
geno_matrix <- as.double(data_autosomal)
geno_tensor <- geno_matrix/2 # alternative approach: keras::to_categorical(geno_matrix)

# Randomly split into the training and the validation set
n_rows <- dim(geno_tensor)[1]
train_idx <- sample(1:n_rows, size = 0.8 * n_rows, replace = F) 
train_data <- geno_tensor[train_idx, ]
valid_data <- geno_tensor[-train_idx, ]
```

## Define the architecture
Here, we define the architecture `r emo::ji('cityscape')` of our autoencoder. Autoencoders are symmetrical creatures, like `r emo::ji('butterfly')`. It implies that the *decoder* is the reversal of the *encoder*, symmetrical about the low-D latent representation layer (a.k.a bottleneck `r emo::ji('bottle')`, in our case 2D). Some dropout layers were added for regularization, i.e. to prevent overfitting.

```{r, cache=F}
input_layer <- layer_input(shape = dim(train_data)[2])
encoder <- 
  input_layer %>% 
  layer_dense(units = 1500, activation = act) %>% 
  layer_batch_normalization() %>% # accelerate the training and make it more stable (a trick of the trade:-)
  layer_dropout(rate = 0.2) %>% 
  layer_dense(units = 500, activation = act) %>%
  layer_dropout(rate = 0.1) %>%
  layer_dense(units = 25, activation = act) %>%
  layer_dense(units = 2) # bottleneck

decoder <- 
  encoder %>% 
  layer_dense(units = 25, activation = act) %>% 
  layer_dropout(rate = 0.2) %>% 
  layer_dense(units = 500, activation = act) %>%
  layer_dropout(rate = 0.1) %>%
  layer_dense(units = 1500, activation = act) %>%
  layer_dense(units = dim(train_data)[2], activation = "sigmoid")

autoencoder_model <- keras_model(inputs = input_layer, outputs = decoder)

autoencoder_model %>% compile(
  loss = loss_fn,
  optimizer = 'adam',
  metrics = c() # here you specify a list of metrics, like accuracy or AUC when applicable
)
```
Now, we can `r emo::ji('eyes')` how our compiled model looks like:
```{r autoenc_summary, class.output='.smallest', cache=T} 
summary(autoencoder_model)
```

## The training `r emo::ji('runner')` phase
We are ready to train our model now. By setting `shuffle = T`, we make sure the training data will be `r emo::ji('game_die')` re-shuffled `r emo::ji('game_die')` in each epoch and `batch_size = 256` tells `keras` to use 256 samples per gradient update (improves efficiency). We want 20% of the training data to be used for validation at each epoch `validation_split = .2` and we can also specify some custom callback functions to, e.g. introduce custom early stopping `r emo::ji('stop_sign')` criteria.

```{r, autoencoder_train, message=FALSE}
history <- autoencoder_model %>% fit(
  x = train_data, 
  y = train_data, 
  epochs = 60, 
  shuffle = T, 
  batch_size = 256,
  validation_split = .2
  #callbacks = list(checkpoint, early_stopping)
)
plot(history) + theme_bw()
```

Now the model has been trained, loss and accuracy are evaluated on both the initial training data at each epoch `r emo::ji('game_die')` split into the new training (80%) and the new test (20%) set. 

### Encoder
Following the training phase, we will build `r emo::ji('building_construction')` the encoder.

```{r}
autoencoder_weights <- autoencoder_model %>% keras::get_weights()
keras::save_model_weights_hdf5(object = autoencoder_model, 
                               filepath = './autoencoder_weights.hdf5', 
                               overwrite = TRUE)

encoder_model <- keras_model(inputs = input_layer, outputs = encoder)
encoder_model %>% keras::load_model_weights_hdf5(filepath = "./autoencoder_weights.hdf5", 
                                                 skip_mismatch = TRUE,
                                                 by_name = T)

encoder_model %>% compile(
  loss = loss_fn,
  optimizer = 'adam',
  metrics = c('MeanSquaredError')
)
```

### Embedding original data
Now, original data can be embedded in the low-dimensional space using the encoder.

```{r}
embeded_points <- 
  encoder_model %>% 
  keras::predict_on_batch(x = geno_tensor)
```

## Final results
Now, we can see how the embeddings compare with the MDS approach.

```{r fig.show = F}
embedded <- data.frame(embeded_points[,1:2], 
                       pop = data_autosomal@phdata$population, 
                       type='emb')
mds <- cbind(ibs, type='mds')
colnames(mds) <- c('x', 'y', 'pop', 'type')
colnames(embedded) <- c('x', 'y', 'pop', 'type')
dat <- rbind(embedded, mds)
dat %>% ggplot(mapping = aes(x=x, y=y, col=pop)) + 
  geom_point() +
  facet_wrap(~type, scales = "free") +
  theme_bw()
```
Are the results produced by autoencoder better? I think it still has problems separating two non-homogenous clusters but it seems that the resolution achieved on the big clumps is better. Probably we should measure this in a more objective way, but since you know how to make autoencoders in R, you can do all sorts of experiments `r emo::ji('mag_right')` now! Good luck with your future endavours.

# Tasks and questions

## Training phase
* Why both `x` and `y` have the same value?
* Try using external validation data instead. Set `validation_data = list(valid_data, valid_data)` that will override the `validation_split` argument. What happens? Remember that, with every epoch, there may occur some `r emo::ji('droplet')`leakage`r emo::ji('droplet')` of the data from the validation set to the training set via network weights...
* What happens if we do not shuffle?
* How does increasing the number of epochs affect model's performance? Try, e.g. 120 epochs.
* Introduce an early stopping criterion using callback function.
A custom callback function may look like:
```{r}
# checkpoint <- callback_model_checkpoint(
#   filepath = "model.hdf5", 
#   save_best_only = TRUE, 
#   period = 1,
#   verbose = 1
# )
# 
# early_stopping <- callback_early_stopping(patience = 5)
```

## More experiments
* Try using autoencoder to reduce dimensionality to more dimensions than 2, say 20 and apply PCA on this latent space to visualize it in 2D. Did you get better resolution? (non-linear -> linear reduction)

* Can you think of using autoencoder on a latent space obtained with MDS or PCA (linear -> non_linear reduction).

* What about chaining two non-linear methods, e.g. [UMAP](https://cran.r-project.org/web/packages/umap/vignettes/umap.html) on top of autoencoder?

# Reproducibility note

Experimental conditions:

* Moon phase: `r emo::moon(lubridate::today())`.
* Sun in the Zodiac sign of: `r emo::ji('bow_and_arrow')`.
* Chinese year of: `r emo::ji('rat')`.
* Recorded average strength of electromagic field: $0.74\mu M$ `r emo::ji('mage')`.
* Witcher `r emo::ji('wolf')` on duty: Geralt of Rivia. 

```{r echo=F, message=F, include=F, eval=T}
#renv::snapshot()
```
<!-- --------------------- Do not edit this and below ---------------------- -->

```{r,echo=FALSE,child="assets/footer-lab.Rmd"}
```

```{r,eval=FALSE,echo=FALSE}
# manually run this to render this document to HTML
rmarkdown::render("lab.Rmd")
# manually run this to convert HTML to PDF
#pagedown::chrome_print("lab.html",output="lab.pdf")
```
